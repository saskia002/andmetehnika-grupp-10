#networks:
#    iceberg_network:
#        driver: bridge

services:
    minio:
        image: minio/minio:RELEASE.2024-10-13T13-34-11Z
        container_name: minio
        ports:
            - "9000:9000" # MinIO API
            - "9901:9001" # MinIO Console
        environment:
            MINIO_ROOT_USER: minioadmin
            MINIO_ROOT_PASSWORD: minioadmin
        command: server /data --console-address ":9001"
        volumes:
            - ./.minio_data:/data
        #networks:
        #    iceberg_network:
        #        aliases:
        #            - minio

    iceberg_rest:
        image: tabulario/iceberg-rest:1.6.0
        container_name: iceberg_rest
        ports:
            - "8181:8181"
        environment:
            AWS_ACCESS_KEY_ID: minioadmin
            AWS_SECRET_ACCESS_KEY: minioadmin
            AWS_REGION: us-east-1
            AWS_ENDPOINT: http://minio:9000
            CATALOG__REST__URI: http://iceberg_rest:8181/
            CATALOG__REST__WAREHOUSE: s3://bucket/
            CATALOG__REST__IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
            CATALOG__REST__S3__ENDPOINT: http://minio:9000
            CATALOG__REST__S3__ACCESS-KEY-ID: minioadmin
            CATALOG__REST__S3__SECRET-ACCESS-KEY: minioadmin
        depends_on:
            - minio
        #networks:
        #    iceberg_network:

    iceberg-init:
        image: python:3.12
        container_name: iceberg-init
        depends_on:
            - minio
            - iceberg_rest
        #networks:
        #    - iceberg_network
        volumes:
            - ./iceberg:/iceberg # contains init files
            - ./requirements.txt:/requirements.txt:ro
        environment:
            PYICEBERG_HOME: /iceberg
            PYICEBERG_CATALOG__REST__URI: http://iceberg_rest:8181/
            PYICEBERG_CATALOG__REST__WAREHOUSE: s3://bucket/
            PYICEBERG_CATALOG__REST__IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
            PYICEBERG_CATALOG__REST__S3__ENDPOINT: http://minio:9000
            PYICEBERG_CATALOG__REST__S3__ACCESS-KEY-ID: minioadmin
            PYICEBERG_CATALOG__REST__S3__SECRET-ACCESS-KEY: minioadmin
        working_dir: /iceberg
        command: >
            bash -c "
              echo 'Installing dependencies...' &&
              pip install -r /requirements.txt &&
              echo 'Waiting for iceberg...' &&
              sleep 10 &&
              python init_s3_bucket.py &&
              python init_duck_db.py &&
              python init_iceberg.py
            "

    airflow-db:
        container_name: airflow-db
        image: postgres:16.4
        #depends_on:
        #    - iceberg-init
        environment:
            POSTGRES_USER: airflow
            POSTGRES_PASSWORD: airflow
            POSTGRES_DB: airflow
            PGUSER: airflow
            PGPASSWORD: airflow
            PGDATABASE: airflow
        ports:
            - "5432:5432"
        volumes:
            - ./pgdata_airflow:/var/lib/postgresql/data
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U airflow"]
            interval: 10s
            retries: 5
        restart: always

    airflow-webserver:
        image: apache/airflow:2.8.1
        container_name: airflow-webserver
        command: webserver
        environment:
            AIRFLOW__CORE__EXECUTOR: LocalExecutor
            AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
            AIRFLOW__CORE__LOAD_EXAMPLES: "false"
            AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth"
            _PIP_ADDITIONAL_REQUIREMENTS: "apache-airflow-providers-mongo pymongo yfinance==0.2.66 multitasking==0.0.11"
        ports:
            - "8080:8080"
        volumes:
            - ./dags:/opt/airflow/dags
            - ./logs:/opt/airflow/logs
            - ./plugins:/opt/airflow/plugins
            - /var/run/docker.sock:/var/run/docker.sock
        depends_on:
            - airflow-init
            - airflow-db
            - iceberg-init
        restart: always

    airflow-scheduler:
        image: apache/airflow:2.8.1
        container_name: airflow-scheduler
        command: scheduler

        environment:
            AIRFLOW__CORE__EXECUTOR: LocalExecutor
            AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
            AIRFLOW__CORE__LOAD_EXAMPLES: "false"
            _PIP_ADDITIONAL_REQUIREMENTS: "apache-airflow-providers-mongo pymongo yfinance==0.2.66 multitasking==0.0.11 requests"
            CLICKHOUSE_USER: etl
            CLICKHOUSE_PASSWORD: pass
        volumes:
            - ./dags:/opt/airflow/dags
            - ./logs:/opt/airflow/logs
            - ./plugins:/opt/airflow/plugins
            - /var/run/docker.sock:/var/run/docker.sock
        depends_on:
            - airflow-init
            - airflow-db
            - iceberg-init
        restart: always

    airflow-init:
        build:
            context: ./dags # Build context is the current directory
            dockerfile: Dockerfile # Use the Dockerfile in this directory to build the image
        container_name: airflow-init
        entrypoint: /bin/bash
        command:
            - -c
            - |
                airflow db init
                airflow users create \
                  --username airflow \
                  --password airflow \
                  --firstname Admin \
                  --lastname User \
                  --role Admin \
                  --email admin@example.com
        environment:
            AIRFLOW__CORE__EXECUTOR: LocalExecutor
            AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
            _PIP_ADDITIONAL_REQUIREMENTS: "yfinance==0.2.66 multitasking==0.0.11"
        volumes:
            - ./dags:/opt/airflow/dags
            - ./logs:/opt/airflow/logs
            - ./plugins:/opt/airflow/plugins
        depends_on:
            - airflow-db
            - iceberg-init
        #restart: always

    clickhouse:
        image: clickhouse/clickhouse-server:24.8
        container_name: clickhouse
        restart: always
        ports:
            - "8123:8123" # HTTP
            - "9002:9000" # TCP
        environment:
            - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
        volumes:
            - clickhouse_data:/var/lib/clickhouse
            - ./clickhouse/init:/docker-entrypoint-initdb.d:ro
        healthcheck:
            test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
            interval: 10s
            timeout: 5s
            retries: 5
            start_period: 30s
        depends_on:
            - iceberg-init

    clickhouse-init-users:
        image: clickhouse/clickhouse-server:24.8
        container_name: clickhouse-init-users
        depends_on:
            clickhouse:
                condition: service_healthy
        command: >
            bash -c "
            echo 'Waiting for ClickHouse to be ready...' &&
            sleep 5 &&
            until clickhouse-client --host clickhouse --port 9000 --query 'SELECT 1' > /dev/null 2>&1; do
              echo 'Waiting for ClickHouse...' &&
              sleep 2
            done &&
            echo 'Ensuring users exist...' &&
            clickhouse-client --host clickhouse --port 9000 --multiquery --query \"
            CREATE USER IF NOT EXISTS etl IDENTIFIED BY 'pass';
            GRANT SELECT, INSERT, CREATE, CREATE DATABASE ON *.* TO etl;
            CREATE USER IF NOT EXISTS dbt_user IDENTIFIED BY 'dbt_pass';
            GRANT SELECT, INSERT, CREATE, CREATE DATABASE, ALTER, DROP ON *.* TO dbt_user;
            \" &&
            echo 'Users verified successfully!' &&
            clickhouse-client --host clickhouse --port 9000 --user dbt_user --password dbt_pass --query 'SELECT 1' &&
            echo 'dbt_user authentication test passed!'
            "
        restart: "on-failure"

    # --- dbt initialization & user setup ---
    dbt:
        # dbt container for building and testing ClickHouse models
        build:
            context: ./dbt # Build context is the current directory
            dockerfile: Dockerfile # Use the Dockerfile in this directory to build dbt image
        container_name: dbt
        depends_on:
            - clickhouse # Ensure ClickHouse starts before dbt
            - clickhouse-init-users # Ensure users are created before dbt
        volumes:
            - ./dbt:/dbt # Mount local dbt project directory
        working_dir: /dbt # Set dbt project directory as the working directory
        tty: true # Keep container running for interactive dbt commands

volumes:
    clickhouse_data:
